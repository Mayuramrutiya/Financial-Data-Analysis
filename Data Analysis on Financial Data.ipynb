{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip the dropped funds data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_files(PATH): # Function requires path to zip folder as a argument\n",
    "    with zipfile.ZipFile(PATH, 'r') as zip_file:\n",
    "        # Function will create Unzipped folder within working directory and extract all files in Unzipped folder\n",
    "        zip_file.extractall('Unzipped') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming zip folder is in same directory as of this notebook, if not then need to pass correct path\n",
    "unzip_files(\"HF Archives.zip\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the unzipped folder and load latest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursively iterate throguth the unzipped folder and record all timestamps and its parent folder path\n",
    "pathlist = Path(\"Unzipped\").glob('**/*')\n",
    "Times= []\n",
    "for path in pathlist:\n",
    "     if os.path.splitext(path)[1] == '.txt':\n",
    "         with open(path , 'r') as f:\n",
    "             time =  f.read()\n",
    "             Times.append((str(path.parents[0]) , datetime.strptime(time, \"%Y%m%d\")))\n",
    "\n",
    "# return latest timestamps and latest funds directory path\n",
    "funds_timstamp = [x[1] for x in Times]\n",
    "latest_timestamp = max(funds_timstamp)\n",
    "latest_timestamp_index = funds_timstamp.index(latest_timestamp)\n",
    "latest_funds_parent_path = Times[latest_timestamp_index][0]\n",
    "\n",
    "# Loop through all the csv files in latest funds directory and load the data\n",
    "dataframs = []\n",
    "dataframe_names= []\n",
    "latest_fund_data = Path(latest_funds_parent_path).glob('**/*.csv')\n",
    "for data_path in latest_fund_data:\n",
    "    dataframe_names.append(data_path.stem)\n",
    "    temp_df = pd.read_csv(data_path)\n",
    "    dataframs.append(temp_df)\n",
    "    \n",
    "# Create Seperate Dataframs and display confirmation message with loaded dataframs name   \n",
    "for i in range(len(dataframs)):\n",
    "    globals()[f\"DF_{dataframe_names[i]}\"] = dataframs[i]\n",
    "    print(f\"Letest DF_{dataframe_names[i]} data has been loaded! It has {dataframs[i].shape[0]} rows and {dataframs[i].shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Date column from object to datetime datatype \n",
    "DF_HF_ROR['Date'] = pd.to_datetime(DF_HF_ROR['Date'])\n",
    "DF_HF_ASSETS['Date']= pd.to_datetime(DF_HF_ASSETS['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1) What are the top and bottom ITD performers:\n",
    "       - By Cumulative ROR?\n",
    "       - By Annualized ROR (Minimum 12 months of data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Cumulative ROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by Fund_ID and Date\n",
    "filtered_df = DF_HF_ROR.sort_values(by=['Fund_ID','Date'])\n",
    "# calculate the cumulative ROR\n",
    "filtered_df['Interest_factor'] = (filtered_df['Performance']/100 +1)\n",
    "filtered_df['Cumulative_ret'] = filtered_df.groupby(['Fund_ID']).Interest_factor.cumprod()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return ITB Cumulative ROR for each fund  by getting latest row in each fund and sort in decending order\n",
    "Cumulative_ret_df = filtered_df.loc[filtered_df.groupby('Fund_ID').Date.idxmax(), ['Fund_ID','Date', 'Cumulative_ret']]\n",
    "Cumulative_ret_df.sort_values(by=['Cumulative_ret'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cumulative_ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print The result to console\n",
    "Top_ITD_performer_by_cumulative_ROR = Cumulative_ret_df.head(1)\n",
    "print(f'Top ITD Performer by Cumulative ROR is:\\n')\n",
    "print(f\"Fund ID : {Top_ITD_performer_by_cumulative_ROR['Fund_ID'].values}\")\n",
    "print(f\"Cumulative ROR : {'{:.2%}'.format(Top_ITD_performer_by_cumulative_ROR['Cumulative_ret'].values[0])}\\n\")\n",
    "Bottom_ITD_performer_by_cumulative_ROR = Cumulative_ret_df.tail(1)\n",
    "print(f'Bottom ITD Performer by Cumulative ROR is:\\n')\n",
    "print(f\"Fund ID : {Bottom_ITD_performer_by_cumulative_ROR['Fund_ID'].values}\")\n",
    "print(f\"Cumulative ROR : {'{:.2%}'.format(Bottom_ITD_performer_by_cumulative_ROR['Cumulative_ret'].values[0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.loc[filtered_df['Fund_ID']== Top_ITD_performer_by_cumulative_ROR['Fund_ID'].values[0], ['Date','Cumulative_ret']].plot( x='Date', y='Cumulative_ret', title = 'Top perfomer fund\\'s cumulative return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.loc[filtered_df['Fund_ID']== Bottom_ITD_performer_by_cumulative_ROR['Fund_ID'].values[0], ['Date','Cumulative_ret']].plot( x='Date', y='Cumulative_ret', title = 'Bottom perfomer fund\\'s cumulative return')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Annualized ROR (Minimum 12 months of Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data with fund that has minimum 12 months of data\n",
    "df =DF_HF_ROR['Fund_ID'].value_counts()\n",
    "funds_with_minimum_36_months = df[df >= 12].index\n",
    "f_df = DF_HF_ROR[DF_HF_ROR['Fund_ID'].isin(funds_with_minimum_36_months)]\n",
    "f_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by Fund_ID and Date\n",
    "f_df = f_df.sort_values(by=['Fund_ID','Date'])\n",
    "# calculate the cumulative ROR\n",
    "f_df['Interest_factor'] = (f_df['Performance']/100 +1)\n",
    "f_df['Cumulative_ret (ITD)'] = f_df.groupby(['Fund_ID']).Interest_factor.cumprod() -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with ITD cumulative ror for each funds \n",
    "Cumulative_ret_df = f_df.loc[f_df.groupby('Fund_ID').Date.idxmax(), ['Fund_ID','Date', 'Cumulative_ret (ITD)']]\n",
    "Cumulative_ret_df.sort_values(by=['Fund_ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Fund_ID as a index for cumulative df, it will be easy to slice and concat dataframe\n",
    "Cumulative_ret_df.set_index('Fund_ID' , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datafram with number of periods of each fund and set fund_id as index\n",
    "number_of_periods = f_df.groupby('Fund_ID').agg({'Fund_ID': 'size'}).rename(columns={'Fund_ID': 'Number_of_Periods'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Annualized dataframe\n",
    "Annualized_df = pd.concat([Cumulative_ret_df,number_of_periods] , axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ITD annualized ROR for funds (Minimum 12 months data)\n",
    "Annualized_df['Annualized ROR (ITD)'] = ((1+ Annualized_df['Cumulative_ret (ITD)'])**(12/Annualized_df['Number_of_Periods']))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print The result to console\n",
    "Top_ITD_performer_by_Annualized_ROR = Annualized_df.sort_values('Annualized ROR (ITD)', ascending=False).head(1)\n",
    "print(f'Top ITD Performer by Annualized ROR is:\\n')\n",
    "print(f\"Fund ID : {Top_ITD_performer_by_Annualized_ROR.index.values}\")\n",
    "print(f\"Annualized ROR : {'{:.2%}'.format(Top_ITD_performer_by_Annualized_ROR['Annualized ROR (ITD)'].values[0])}\\n\")\n",
    "Bottom_ITD_performer_by_Annualized_ROR = Annualized_df.sort_values('Annualized ROR (ITD)', ascending=False).tail(1)\n",
    "print(f'Bottom ITD Performer by Annualized ROR is:\\n')\n",
    "print(f\"Fund ID : {Bottom_ITD_performer_by_Annualized_ROR.index.values}\")\n",
    "print(f\"Annualized ROR : {'{:.2%}'.format(Bottom_ITD_performer_by_Annualized_ROR['Annualized ROR (ITD)'].values[0])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the top and bottom 10 YTD performers by ROR of 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered dataframe for year 2018\n",
    "filtered_df = DF_HF_ROR[DF_HF_ROR['Date'].dt.year == 2018].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['Interest_factor'] = (filtered_df['Performance']/100 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note = With the assumption that performace given is Discrete Return\n",
    "top10_2018_ytd_funds = filtered_df.groupby([\"Fund_ID\"]).Interest_factor.prod().nlargest(10).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note = With the assumption that performace given is Discrete Return\n",
    "Bottom10_2018_ytd_funds = filtered_df.groupby([\"Fund_ID\"]).Interest_factor.prod().nsmallest(10).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add YTD ROR percentage column\n",
    "top10_2018_ytd_funds['YTD_ROR'] = (top10_2018_ytd_funds['Interest_factor'] -1 )*100\n",
    "Bottom10_2018_ytd_funds['YTD_ROR'] = (Bottom10_2018_ytd_funds['Interest_factor'] -1 )*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Performer for 2018 by YTD ROR\n",
    "x_labels = top10_2018_ytd_funds['Fund_ID'].values\n",
    "YTD_Return = top10_2018_ytd_funds['YTD_ROR']\n",
    "# Plot the figure.\n",
    "plt.figure(figsize=(10, 7))\n",
    "ax = YTD_Return.plot(kind=\"bar\")\n",
    "ax.set_title(\"Top 10 YTD ROR for 2018\")\n",
    "ax.set_xlabel(\"Fund ID\")\n",
    "ax.set_ylabel(\"YTD ROR (%)\")\n",
    "ax.set_xticklabels(x_labels)\n",
    "rects = ax.patches\n",
    "\n",
    "# Make PCT value as labels.\n",
    "labels = pd.Series([\"{0:.2f}%\".format(val) for val in top10_2018_ytd_funds['YTD_ROR']], index = top10_2018_ytd_funds.index)\n",
    "\n",
    "# add lables to graph\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width() / 2, height +0.5 , label, ha=\"center\", va=\"bottom\"\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom 10 Performer for 2018 by YTD ROR\n",
    "x_labels = Bottom10_2018_ytd_funds['Fund_ID'].values\n",
    "YTD_Return = Bottom10_2018_ytd_funds['YTD_ROR']\n",
    "# Plot the figure.\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = YTD_Return.plot(kind=\"bar\")\n",
    "ax.set_title(\"Bottom 10 YTD ROR for 2018\")\n",
    "ax.set_xlabel(\"Fund ID\")\n",
    "ax.set_ylabel(\"YTD ROR (%)\")\n",
    "ax.set_xticklabels(x_labels)\n",
    "\n",
    "rects = ax.patches\n",
    "\n",
    "# Make PCT value as labels\n",
    "labels = pd.Series([\"{0:.2f}%\".format(val) for val in Bottom10_2018_ytd_funds['YTD_ROR']], index = Bottom10_2018_ytd_funds.index)\n",
    "\n",
    "\n",
    "# add lables to graph\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width() / 2, height - 3, label, ha=\"center\", va=\"bottom\"\n",
    "    )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assuming an annualized risk free return of 2%, which funds show the best risk adjusted returns (minimum 36 months of data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data with fund that has minimum 36 months of data\n",
    "df =DF_HF_ROR['Fund_ID'].value_counts()\n",
    "funds_with_minimum_36_months = df[df >= 36].index\n",
    "f_df = DF_HF_ROR[DF_HF_ROR['Fund_ID'].isin(funds_with_minimum_36_months)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the annualized return of funds (Minimum 36 months of data)\n",
    "\n",
    "# Sort data by Fund_ID and Date\n",
    "f_df = f_df.sort_values(by=['Fund_ID','Date'])\n",
    "# calculate the cumulative ROR\n",
    "f_df['Interest_factor'] = (f_df['Performance']/100 +1)\n",
    "f_df['Cumulative_ret (ITD)'] = f_df.groupby(['Fund_ID']).Interest_factor.cumprod()-1\n",
    "# create dataframe with ITD cumulative ror for each funds \n",
    "Cumulative_ret_df = f_df.loc[f_df.groupby('Fund_ID').Date.idxmax(), ['Fund_ID','Date', 'Cumulative_ret (ITD)']]\n",
    "Cumulative_ret_df.sort_values(by=['Fund_ID'], inplace=True)\n",
    "# Set Fund_ID as a index for cumulative df, it will be easy to slice and concat datafram\n",
    "Cumulative_ret_df.set_index('Fund_ID' , inplace = True)\n",
    "# create datafram with number of periods of each fund and set fund_id as index\n",
    "number_of_periods = f_df.groupby('Fund_ID').agg({'Fund_ID': 'size'}).rename(columns={'Fund_ID': 'Number_of_Periods'})\n",
    "# create Annualized dataframe\n",
    "Annualized_df = pd.concat([Cumulative_ret_df,number_of_periods] , axis =1)\n",
    "# create ITD annualized ROR for funds (Minimum 12 months data)\n",
    "Annualized_df['Annualized ROR (ITD)'] = ((1+ Annualized_df['Cumulative_ret (ITD)'])**(12/Annualized_df['Number_of_Periods']))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the annualized Standrd Deviation of funds (Minimum 36 months of data)\n",
    "\n",
    "Annualized_STD = f_df.groupby('Fund_ID').agg({'Interest_factor': 'std'}).rename(columns={'Interest_factor': 'Standard deviation'})\n",
    "Annualized_STD['Annualized Standard deviation'] = Annualized_STD['Standard deviation'] * np.sqrt(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df[f_df['Fund_ID']== 185684]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized risk free return rate is 2%\n",
    "Annualized_Risk_Free_ROR = 2/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Annualized risk adjusted return\n",
    "Annualized_df = pd.concat([Annualized_df,Annualized_STD] , axis =1 )\n",
    "Annualized_df['Risk_Adjusted_return(Sharp Ratio)'] = (Annualized_df['Annualized ROR (ITD)'] - Annualized_Risk_Free_ROR)/Annualized_df['Annualized Standard deviation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the datafram by Risk_Adjusted_return(Sharp Ratio) in descending order\n",
    "Annualized_df.sort_values('Risk_Adjusted_return(Sharp Ratio)',ascending=False,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best risk adjusted return fund\n",
    "Best_Fund_By_RAR = Annualized_df.head(1)\n",
    "print(f'Best Fund by Annualized Risk Adjusted Return (Sharp Ratio) is:\\n')\n",
    "print(f\"Fund ID : {Best_Fund_By_RAR.index.values}\")\n",
    "print(f\"Annualized Risk Adjusted Return (Sharp Ratio) : {Best_Fund_By_RAR['Risk_Adjusted_return(Sharp Ratio)'].values[0]}\")\n",
    "print(f\"Annualized Standard Deviation (volatility) : {Best_Fund_By_RAR['Annualized Standard deviation'].values[0]}\")\n",
    "print(f\"Annualized ROR : {'{:.2%}'.format(Best_Fund_By_RAR['Annualized ROR (ITD)'].values[0])}\")\n",
    "print(f\"Number of Months : {Best_Fund_By_RAR['Number_of_Periods'].values[0]}\")\n",
    "print(f\"Cumulative ROR : {'{:.2%}'.format(Best_Fund_By_RAR['Cumulative_ret (ITD)'].values[0])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a composite index of the best performing 10 funds over 2018 and track the performance over the year.\n",
    "- Assume the portfolio is reweighted at the beginning of each month based on the prior month’s top performers\n",
    "- Assume equal weighting each month (each fund is 10% of the portfolio)\n",
    "- What would be the expected rate of return at the end of 2018?\n",
    "- Which fund contributed the most to the composite’s total rate of return? What its contribution to the total contribution of the pro-forma index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data\n",
    "filtered_df = DF_HF_ROR[(DF_HF_ROR['Date'] >= '12/31/2017') & (DF_HF_ROR['Date'] <= '12/31/2018')].copy()\n",
    "filtered_df = filtered_df.sort_values(['Fund_ID', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return Previous months performace \n",
    "filtered_df['Previous_month_performance'] = filtered_df.groupby(['Fund_ID']).Performance.shift(1)\n",
    "\n",
    "# Drop null values from previous month performance column: \n",
    "#This will drop all the rows for December 2017 and only keep rows for 2018\n",
    "filtered_df = filtered_df[filtered_df['Previous_month_performance'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Composite index\n",
    "# -- This will return top 10 best performing funds based on previous month performance for each month\n",
    "# -- Using MultipleIndex to store Fund_ID, Date, and current month performance \n",
    "Composite_Index = filtered_df.set_index(['Fund_ID' , 'Date', 'Performance']).groupby('Date').Previous_month_performance.nlargest(10)\n",
    "Composite_Index = Composite_Index.reset_index(level=[0,1,1])\n",
    "Composite_Index['Performance'] = Composite_Index.index\n",
    "Composite_Index['Performance_decimal'] = Composite_Index['Performance']/100\n",
    "\n",
    "Composite_Index.reset_index(drop = True , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Weights column : in our case we have equal weight of 10%\n",
    "Composite_Index['Weight_PCT'] = 0.10\n",
    "\n",
    "# Calculate the weighted return\n",
    "Composite_Index['Weighted_Performance_decimal'] = (Composite_Index['Performance_decimal']) * Composite_Index['Weight_PCT']\n",
    "Composite_Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return Monthly performance of composite index based on weighted return of the funds\n",
    "Composite_Index_monthly_perfomance = Composite_Index.groupby('Date').Weighted_Performance_decimal.sum().reset_index()\n",
    "\n",
    "# Expected Rate of return at the end of 2018 for composite index\n",
    "Expected_ROR_2018 = (Composite_Index_monthly_perfomance.sum()/12)\n",
    "print(f\"Expected (Average) ROR at the end of 2018 for the composite fund is: {'{:.2%}'.format(Expected_ROR_2018.values[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite Index Monthly Performance\n",
    "Composite_Index_monthly_perfomance.plot.line(x = 'Date' , y ='Weighted_Performance_decimal', title = 'Composite Index Monthly Performance')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Composite_Index_monthly_perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total ROR at the end of 2018\n",
    "Composite_Index_monthly_perfomance['Interest_factor'] = Composite_Index_monthly_perfomance['Weighted_Performance_decimal']+1\n",
    "Total_ROR_at_the_end_2018 = (Composite_Index_monthly_perfomance.Interest_factor.prod()-1)*100\n",
    "print (f'Total ROR at the end of 2018 for the composite index is : {\"{0:.2f}%\".format(Total_ROR_at_the_end_2018)}')\n",
    "\n",
    "#fund contributed the most to the composite’s total rate of return\n",
    "\n",
    "Composite_Index['Interest_factor'] = Composite_Index['Weighted_Performance_decimal'] + 1\n",
    "\n",
    "Fund_Contribution  = Composite_Index.groupby('Fund_ID').Interest_factor.prod()\n",
    "Fund_Contribution = Fund_Contribution.reset_index()\n",
    "Fund_Contribution = Fund_Contribution.sort_values('Interest_factor', ascending = False)\n",
    "Fund_Contribution.reset_index(drop = True, inplace =True)\n",
    "Fund_Contribution\n",
    "\n",
    "Fund_Contributted_the_most = Fund_Contribution.head(1)\n",
    "print(f'The fund contributed the most to Composite total ROR is:')\n",
    "print(f\"Fund ID : {Fund_Contributted_the_most['Fund_ID'].values[0]}\")\n",
    "\n",
    "# (Fund_Contribution['Interest_factor'].div(Total_ROR_at_the_en//////////d_2018)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide the Top/Bottom 10 increases in AUM since the beginning of 2018 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filteretd the dataframe to get data since beging of 2018\n",
    "# Assets provided millions.\n",
    "\n",
    "filtered_df = DF_HF_ASSETS[DF_HF_ASSETS['Date'] >= '2017-12-31'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the datafram in ascending order\n",
    "filtered_df.sort_values('Date' , ascending = True, inplace = True)\n",
    "\n",
    "# Calculate AMU increses since the beging of 2018\n",
    "Grouped_data=filtered_df.groupby(by='Fund_ID').agg(['first','last'])\n",
    "Grouped_data.columns =  ['Starting Date (End of 2017)', 'Latest Date', 'AUM at start of 2018 (Millions)', 'Latest AUM (Millions)']\n",
    "Grouped_data['AUM Increase (Millions)'] =  Grouped_data['Latest AUM (Millions)'] - Grouped_data['AUM at start of 2018 (Millions)']\n",
    "\n",
    "# Order the datafram by AUM Increases in Descending order and return top and bottom 10 funds\n",
    "Grouped_data.sort_values('AUM Increase (Millions)', ascending = False, inplace = True )\n",
    "Top10_AUM_Increased_since_2018 = Grouped_data.head(10)\n",
    "Bottom10_AUM_Increased_since_2018 = Grouped_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 increase since the beging of 2018\n",
    "Top10_AUM_Increased_since_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = Top10_AUM_Increased_since_2018.index.values\n",
    "YTD_Return = Top10_AUM_Increased_since_2018['AUM Increase (Millions)']\n",
    "# Plot the figure.\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = YTD_Return.plot(kind=\"bar\")\n",
    "ax.set_title(\"Top 10 AUM increases since begning of 2018\")\n",
    "ax.set_xlabel(\"Fund ID\")\n",
    "ax.set_ylabel(\"AUM increase in Millions\")\n",
    "ax.set_xticklabels(x_labels)\n",
    "rects = ax.patches\n",
    "\n",
    "# Make PCT value as labels.\n",
    "labels = pd.Series([\"{0:.2f} M\".format(val) for val in Top10_AUM_Increased_since_2018['AUM Increase (Millions)']], index = Top10_AUM_Increased_since_2018.index)\n",
    "\n",
    "# add lables to graph\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width() / 2, height + 1.5 , label, ha=\"center\", va=\"bottom\"\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom 10 increase since the beging of 2018\n",
    "Bottom10_AUM_Increased_since_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = Bottom10_AUM_Increased_since_2018.index.values\n",
    "YTD_Return = Bottom10_AUM_Increased_since_2018['AUM Increase (Millions)']\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = YTD_Return.plot(kind=\"bar\")\n",
    "ax.set_title(\"Bottom 10 AUM increases since begning of 2018\")\n",
    "ax.set_xlabel(\"Fund ID\")\n",
    "ax.set_ylabel(\"AUM increase in Millions\")\n",
    "ax.set_xticklabels(x_labels)\n",
    "\n",
    "rects = ax.patches\n",
    "\n",
    "# Make PCT value as labels\n",
    "labels = pd.Series([\"{0:.2f}M\".format(val) for val in Bottom10_AUM_Increased_since_2018['AUM Increase (Millions)']], index = Bottom10_AUM_Increased_since_2018.index)\n",
    "\n",
    "\n",
    "# add lables to graph\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width() / 2, height - 2500, label,ha=\"center\", va=\"bottom\"\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why AUM Tracking is Important ?\n",
    "\n",
    "##### AUM refers to the total market value of assets being managed fund manager or institution. the exact definition of AUM varies by institution; some include bank deposits, mutual funds, and cash in their computation, while others only consider the funds that investors have given an advisor to trade on their behalf.\n",
    "\n",
    "##### AUM reflects management performance and size. Oftenly, a fund's management fees and expenses are calculated as a percentage of AUM. So, it is imperative to track AUM increases. Many firm will monitor AUM as it relates to determining the strength of the company. Investment companies also use AUM as a marketing tool to attract new investors. AUM can help investors get an indication of the size of a company's operations relative to its competitors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segment managers into appropriate buckets by AUM size and evaluate performance (ROR) across the segments. What results and general trends do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return latest AUM of the Funds\n",
    "latest_fund_AUM = DF_HF_ASSETS.sort_values(['Fund_ID','Date']).groupby('Fund_ID').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order Funds by AUM by descending order\n",
    "latest_fund_AUM.sort_values('Assets', ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Funds based on AUM\n",
    "bins = [0,100,200,300,400,500,600,700,800,900,1000,90000]\n",
    "labels = [\"AUM Under 100 Millions\",\"AUM Between 100 to 200 Millions\",\"AUM Between 200 to 300 Millions\",\"AUM Between 300 to 400 Millions\",\"AUM Between 400 to 500 Millions\",\"AUM Between 500 to 600 Millions\",\"AUM Between 600 to 700 Millions\", \"AUM Between 700 to 800 Millions\", \"AUM Between 800 to 900 Millions\",\"AUM Between 900 to 1000 Millions\",\"AUM Over 1000 Millions\"]\n",
    "\n",
    "latest_fund_AUM['Segments'] = pd.cut(latest_fund_AUM['Assets'],bins=bins, labels=labels , include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AUM can change beacuse of two reasons 1) Price changes 2) New investments\n",
    "##### I could have calculated performance from AUM but to do so i need to assume that chnages in AUM is only beacase of price chnage and there is no new investments, which is not realistic assumption and there is no explicit instruction provided so decided to use ROR data (Cumulative ROR of the funds) for performance evaluation.  \n",
    "##### AUM dataframe has less funds then ROR dataframe so using left outer join (Left table is AUM Data) to return all the funds from AUM dataframe and only the matched funds from cumulative dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative ROR of Funds \n",
    "f_df = DF_HF_ROR.copy()\n",
    "# Sort data by Fund_ID and Date\n",
    "f_df = f_df.sort_values(by=['Fund_ID','Date'])\n",
    "# calculate the cumulative ROR\n",
    "f_df['Interest_factor'] = (f_df['Performance']/100 +1)\n",
    "f_df['Cumulative_ret (ITD)'] = f_df.groupby(['Fund_ID']).Interest_factor.cumprod()-1\n",
    "# create dataframe with ITD cumulative ROR for each funds \n",
    "Cumulative_ret_df = f_df.loc[f_df.groupby('Fund_ID').Date.idxmax(), ['Fund_ID','Date', 'Cumulative_ret (ITD)']]\n",
    "Cumulative_ret_df.sort_values(by=['Fund_ID'], inplace=True)\n",
    "# Set Fund_ID as a index for cumulative df, it will be easy to slice and concat datafram\n",
    "Cumulative_ret_df.set_index('Fund_ID' , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update column to indetify data is coming from ROR dataframe\n",
    "Cumulative_ret_df.columns =['ROR_Date', 'Cumulative_ROR (ITD)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combine datafram of AUM dataframe and cumulative ROR dataframe to compare perfomance across segements\n",
    "Combine_Df = latest_fund_AUM.join(Cumulative_ret_df , how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Segment Analysis\n",
    " - Segment Size Distribution\n",
    " - Performance Accrose Segemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Size Distribution \n",
    "Combine_Df['Segments'].value_counts(normalize =True).plot( kind = 'bar')\n",
    "Combine_Df['Segments'].value_counts(normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Segments_average_cumulative_ROR = Combine_Df.groupby('Segments')['Cumulative_ROR (ITD)'].mean()\n",
    "print(\"Segment's average cumulative ROR : \")\n",
    "print(Segments_average_cumulative_ROR)\n",
    "print(\"=================================================\")\n",
    "Segments_volatility = Combine_Df.groupby('Segments')['Cumulative_ROR (ITD)'].std()\n",
    "print(\"Segment's volatility: Standard Deviation\")\n",
    "print(Segments_volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segment Observation:\n",
    "\n",
    "##### Distribution:\n",
    "  * Segment size is are very right skewd as majority of funds has AUM under 100 millions (71% of funds)\n",
    "\n",
    "##### Performance across segments:\n",
    "  * AUM between 800 to 900 millions segment has lowest volatility which suggests that their fund managers tend to take less risk. This Segment also has second lowest average cumulative ROR.\n",
    "  \n",
    "  * AUM between 900 to 1000 millions and AUM over 1000 Millions has highest and second highest average cumulative ROR but their fund managers has taken highest and second highest risk respectively.\n",
    "  \n",
    "  * AUM Under 100 Millions has the lowest average cumulative ROR and their fund manager has taken medium risk.\n",
    "  \n",
    "  * Based on above mentioned observations, we can say that Higher AUM funds managers has taken higher risk and scored higher average cumulative ROR while lower AUM funds managers taken medium to low risk and return comparativly lower average cumulative ROR.\n",
    "  \n",
    "  * Since segment size is very right skewed, this observations are not stongly significant. it would have been more significant if segment size distribution was approximately uniform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
